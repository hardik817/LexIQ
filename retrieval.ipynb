{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c322d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "import json\n",
    "import cohere\n",
    "import gradio as gr\n",
    "from keybert import KeyBERT\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from langchain.schema.document import Document\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1acdc6",
   "metadata": {},
   "source": [
    "Load the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4da9855e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachd\\AppData\\Local\\Temp\\ipykernel_30700\\2191836911.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"intfloat/e5-large-v2\")\n"
     ]
    }
   ],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"intfloat/e5-large-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc436b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.load_local(\n",
    "    \"faiss_e5_index1\",\n",
    "    embedding,\n",
    "    allow_dangerous_deserialization=True,\n",
    "    index_name=\"index\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51c77008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachd\\AppData\\Local\\Temp\\ipykernel_30700\\1555928118.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3.2\")\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a8f00",
   "metadata": {},
   "source": [
    "Loading the Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b0d187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = FAISS.load_local(\n",
    "    \"faiss_e5_index1\",\n",
    "    embedding,\n",
    "    allow_dangerous_deserialization=True\n",
    ").as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b4633",
   "metadata": {},
   "source": [
    "To improve the answer quality we will use re-ranking to get better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90b631f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cohere.ClientV2(os.environ.get(\"COHERE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ef91c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohere_rerank(query, docs, top_n=5):\n",
    "    doc_texts = [doc.page_content for doc in docs]\n",
    "    response = co.rerank(query=query, documents=doc_texts, top_n=top_n, model=\"rerank-v3.5\")\n",
    "    reranked = [docs[r.index] for r in response.results]\n",
    "    return reranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55fc6831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_rerank(query):\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    reranked_docs = cohere_rerank(query, retrieved_docs, top_n=5)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in reranked_docs])\n",
    "    prompt = f\"Answer the following legal question in detail using the context below:\\n\\nQuestion: {query}\\n\\nContext:\\n{context}\\n\\nAnswer:\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbc1c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(query,history):\n",
    "    response = answer_with_rerank(query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2110b306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sachd\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* Running on public URL: https://d3a4979c9ecf330f22.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d3a4979c9ecf330f22.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "legal=gr.ChatInterface(\n",
    "    fn=answer_question,\n",
    "    title=\"Legal Question Answering\",\n",
    "    description=\"Ask a legal question and get an answer based on the provided context.\",\n",
    "    examples=[\n",
    "        [\"Can an employer change terms without notifying employees?\"],\n",
    "        [\"What rights do tenants have if their landlord doesnâ€™t make repairs?\"],\n",
    "        [\"Can I challenge this arbitration clause in court?\"]\n",
    "    ]\n",
    ")\n",
    "legal.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b080992d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
